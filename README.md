# Single View 3D Reconstruction

A end-to-end Single View 3D Reconstruction network for my bachelor thesis. 

*by Cheng Jun-Yan*

**Prerequisite:** mainly python3.6, tensorflow1.4.1-gpu, ubuntu16.04, CUDA8.0, opencv2, open3d, OpenEXR, trimesh, blender

![image](https://github.com/chengjunyan1/Single-View-3D-Reconstruction/raw/master/pipeline.png)

This repository is the code for my bachelor thesis (advised by Prof. Min Jiang). The Thesis&Works folder also includes the bachelor thesis, a short version for the first class outstanding bachelor thesis, future improvement ideas, paper readong notes, and the ppt for thesis defense. This work was awarded an outstanding bachelor thesis (first class).

This is not the original version for code used in the bachelor thesis. Some improvements have been made such as replacing the VGG-19 in the view synthesis net with the DenseNet. The size of the network is reduced and the running speed is also improved. However, I didn't have too much experiments for this version. So the reconstruction performance of this version is not guarenteed.

### Idea 

The thesis is written by chinese, so I'll briefly describe my idea here in english. 

The idea is to recover the 3D infomation from the monocular picture stage by stage:

Stage 1 *Visual Synthesis*: Estimating its corresponding binocular pictures which is estimating the photo of the 3D object taken in another viewpoint. 

Stage 2 *Stereo Match*: The depth map could be estimated based on the binocular picture. 

Stage 3 *Point Cloud Completion*: Using the depth map to estimate the 3D point cloud. 

The 3D infomation is gradually recovered by the above steps. The network could be trained end-to-end. Each stage could be viewed as a "prunning" process, the searching space is gradually reduced in each step, thus the learning of this task could be easier. 

*Moreover*, one advantage of this method is that, once this network is trained, it could easily be applied in other kind of sensers. For example, by disabling the view synthesis network, the model could be used for binocular camera, disabling the stereo match network, the network could then be used in Lidar.

## Sample

![image](https://github.com/chengjunyan1/Single-View-3D-Reconstruction/raw/master/sv3dr.png)

This is a sample outputs of my model. gt means groud truth synthesed in blender, estimated means estimated by the model. The last three rows are generated by directly input groud truth depth map which means only use the third stage, point cloud completion, to reconstruct the 3d shape which is close to the groud truth shape. The color of the view synthesis result pictures do not showed correctly because I represent color in [0,255] which should be in [0,1]
